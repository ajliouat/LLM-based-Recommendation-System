{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwyQdg53ag0m5PZDvcepWD"},"kernelspec":{"name":"python3","display_name":"Python 3.12.2 64-bit"},"language_info":{"name":"python","version":"3.12.2"},"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"cells":[{"cell_type":"code","execution_count":null,"source":["# Setup: Install necessary packages in Colab\n","!pip install numpy\n","!pip install --pre onnxruntime-genai  # For CPU\n","!pip install huggingface-hub[cli]\n","!pip install faiss-cpu\n","!pip install sentence-transformers\n","\n","# Download the model for CPU\n","!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-onnx --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/* --local-dir ./cpu_and_mobile\n"],"outputs":[],"metadata":{"id":"A7jx_R906NwC"}},{"cell_type":"code","execution_count":null,"source":["\n","# Data\n","product_data.json = [\n","    {\"id\": 1, \"name\": \"Tesla Model S\", \"description\": \"High-performance electric sedan with long driving range.\"},\n","    {\"id\": 2, \"name\": \"Nissan Leaf\", \"description\": \"Affordable and reliable electric hatchback for daily commutes.\"},\n","    {\"id\": 3, \"name\": \"Toyota Camry\", \"description\": \"Popular mid-size sedan known for its reliability and comfort.\"},\n","    {\"id\": 4, \"name\": \"Chevrolet Bolt\", \"description\": \"Compact electric hatchback with impressive range and technology features.\"},\n","    {\"id\": 5, \"name\": \"Ford Mustang Mach-E\", \"description\": \"Electric SUV with sporty performance and sleek design.\"},\n","    {\"id\": 6, \"name\": \"Honda Accord\", \"description\": \"Well-rounded mid-size sedan with a spacious interior and fuel efficiency.\"},\n","    {\"id\": 7, \"name\": \"BMW i3\", \"description\": \"Compact electric car with a unique design and premium features.\"},\n","    {\"id\": 8, \"name\": \"Audi e-tron\", \"description\": \"Luxury electric SUV with advanced technology and refined craftsmanship.\"},\n","    {\"id\": 9, \"name\": \"Hyundai Kona Electric\", \"description\": \"Affordable electric SUV with impressive driving range.\"},\n","    {\"id\": 10, \"name\": \"Porsche Taycan\", \"description\": \"High-performance electric sports car with cutting-edge technology.\"}\n","]\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Define the search options as a dictionary\n","search_options.json = {\n","    \"max_length\": 1000,\n","    \"num_return_sequences\": 1,\n","    \"temperature\": 0.7,\n","    \"top_k\": 50,\n","    \"top_p\": 0.9,\n","    \"repetition_penalty\": 2.0,\n","    \"do_sample\": True\n","}"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Import libraries\n","import json\n","from typing import List, Dict, Any\n","import numpy as np\n","import time\n","import onnxruntime_genai as og\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","import logging\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","class ContextMemory:\n","    \"\"\"A class to manage the conversation history and provide the relevant context.\"\"\"\n","    def __init__(self, max_history_length: int = 5):\n","        self.memory: List[str] = []\n","        self.max_history_length: int = max_history_length\n","\n","    def update_memory(self, user_prompt: str) -> None:\n","        \"\"\"Updates the conversation history by adding the user prompt and limiting the history length.\"\"\"\n","        self.memory.append(f\"<|user|>\\n{user_prompt}<|end|>\")\n","        if len(self.memory) > self.max_history_length:\n","            self.memory.pop(0)\n","\n","    def get_context(self) -> str:\n","        \"\"\"Retrieves the conversation history as a string.\"\"\"\n","        return \"\".join(self.memory)\n","\n","class EmbeddingProvider:\n","    \"\"\"A class to generate embeddings for text queries using a pre-trained sentence transformer model.\"\"\"\n","    def __init__(self, model_name: str):\n","        self.model = SentenceTransformer(model_name)\n","\n","    def get_embedding(self, query: str) -> np.ndarray:\n","        \"\"\"Generates the embedding for a given text query.\"\"\"\n","        return self.model.encode(query)\n","\n","class FaissIndexer:\n","    \"\"\"A class to create and search a FAISS index for efficient similarity search.\"\"\"\n","    def __init__(self, embedding_size: int):\n","        self.index = faiss.IndexFlatL2(embedding_size)\n","\n","    def add_items(self, item_embeddings: np.ndarray) -> None:\n","        \"\"\"Adds item embeddings to the FAISS index.\"\"\"\n","        self.index.add(item_embeddings)\n","\n","    def search(self, query_embedding: np.ndarray, top_k: int = 3) -> np.ndarray:\n","        \"\"\"Searches the FAISS index for the top-k most similar items to the query embedding.\"\"\"\n","        _, indices = self.index.search(query_embedding, top_k)\n","        return indices[0]\n","\n","class SearchModel:\n","    \"\"\"A class to generate reformulated queries based on user input and conversation history.\"\"\"\n","    def __init__(self, model_path: str, search_options: Dict[str, Any], max_history_length: int = 5):\n","        self.model = og.Model(model_path)\n","        self.tokenizer = og.Tokenizer(self.model)\n","        self.search_options = search_options\n","        self.context_memory = ContextMemory(max_history_length)\n","\n","    def generate_reformulated_query(self, input_text: str) -> str:\n","        \"\"\"Generates a reformulated query based on the user input and conversation history.\"\"\"\n","        conversation_history = self.context_memory.get_context()\n","        self.context_memory.update_memory(input_text)\n","        full_prompt = f\"{conversation_history}<|user|>\\nYou're an assistant that helps capture the evolving intent of the user and reformulate on a question. The user's previous history that may be related to the current input:\\n{conversation_history}\\nThe current user request input is: {input_text}\\nReformulate the current input in the form of a question that captures only the recent request of the user. You need to consider only the relevant parts of the previous search history to capture the user's most recent intent. When taking into consideration the previous history of search the most recent one may be more related to the current user prompt.\\n<|end|><|user|>\\n{input_text}<|end|><|assistant|>\"\n","        input_tokens = self.tokenizer.encode(full_prompt)\n","        params = og.GeneratorParams(self.model)\n","        params.set_search_options(**self.search_options)\n","        params.input_ids = input_tokens\n","        generator = og.Generator(self.model, params)\n","        response_tokens = []\n","        while not generator.is_done():\n","            generator.compute_logits()\n","            generator.generate_next_token()\n","            new_token = generator.get_next_tokens()[0]\n","            response_tokens.append(new_token)\n","        reformulated_query = self.tokenizer.decode(response_tokens)\n","        reformulated_query = reformulated_query.split('<|assistant|>')[-1].strip()\n","        return reformulated_query\n","\n","def load_json_file(file_path: str) -> Dict[str, Any]:\n","    \"\"\"Loads a JSON file and returns its contents as a dictionary.\"\"\"\n","    with open(file_path, 'r') as file:\n","        return json.load(file)\n","\n","def load_product_data(file_path: str) -> List[Dict[str, Any]]:\n","    \"\"\"Loads product data from a JSON file.\"\"\"\n","    return load_json_file(file_path)\n","\n","def load_search_options(file_path: str) -> Dict[str, Any]:\n","    \"\"\"Loads search options from a JSON file.\"\"\"\n","    return load_json_file(file_path)\n","\n","def process_user_input(user_input: str, conv_model: SearchModel, embedding_provider: EmbeddingProvider, faiss_indexer: FaissIndexer, products: List[Dict[str, Any]]) -> None:\n","    \"\"\"Processes a single user input and generates a response.\"\"\"\n","    start_time = time.time()\n","    try:\n","        reformulated_query = conv_model.generate_reformulated_query(user_input)\n","        query_embedding = embedding_provider.get_embedding(reformulated_query).reshape(1, -1)\n","        end_time = time.time()\n","        execution_time = end_time - start_time\n","\n","        print(\"Assistant:\", reformulated_query)\n","        print(f\"Execution time: {execution_time:.5f} seconds\")\n","\n","        # Print the current state of the conversation history\n","        print(\"Current Conversation History:\")\n","        print(conv_model.context_memory.get_context())\n","\n","        # Retrieve top matching products\n","        top_indices = faiss_indexer.search(query_embedding)\n","        print(f\"User query: {user_input}\")\n","        print(f\"Reformulated query: {reformulated_query}\")\n","        print(\"Top matches:\")\n","        for idx in top_indices:\n","            product = products[idx]\n","            print(f\"  - ID: {product['id']}, Name: {product['name']}, Description: {product['description']}\")\n","\n","    except Exception as e:\n","        logging.error(f\"Error generating response: {str(e)}\")\n","\n","def main():\n","    \"\"\"The main function that orchestrates the search process.\"\"\"\n","    # Load search options and product data\n","    search_options_path = './search_options.json'\n","    product_data_path = './product_data.json'\n","    search_options = load_search_options(search_options_path)\n","    products = load_product_data(product_data_path)\n","    product_descriptions = [product['description'] for product in products]\n","\n","    # Initialize the model and components\n","    model_path = './cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4'\n","    conv_model = SearchModel(model_path, search_options, max_history_length=5)\n","    embedding_provider = EmbeddingProvider('all-mpnet-base-v2')\n","    product_embeddings = np.array([embedding_provider.get_embedding(desc) for desc in product_descriptions])\n","    faiss_indexer = FaissIndexer(embedding_size=product_embeddings.shape[1])\n","    faiss_indexer.add_items(product_embeddings)\n","\n","    # Pre-load models and data\n","    _ = conv_model.generate_reformulated_query(\"Dummy input\")\n","    _ = embedding_provider.get_embedding(\"Dummy query\")\n","    conv_model.context_memory.memory.clear()  # Clear the dummy input from the conversation history\n","\n","    # Generation loop\n","    with ThreadPoolExecutor() as executor:\n","        while True:\n","            user_input = input(\"User: \").strip()\n","            if user_input.lower() == 'exit':\n","                break\n","\n","            if not user_input:\n","                print(\"Please enter a valid input.\")\n","                continue\n","\n","            executor.submit(process_user_input, user_input, conv_model, embedding_provider, faiss_indexer, products)\n","\n","if __name__ == \"__main__\":\n","    main()"],"outputs":[],"metadata":{}}]}