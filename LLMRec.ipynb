{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNyqPNE08AhBMzBW0B/RKn3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajliouat/LLMRec/blob/main/LLMRec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "u5cTjSp5hik0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup: Install necessary packages in Colab\n",
        "!pip install numpy\n",
        "!pip install --pre onnxruntime-genai  # For CPU\n",
        "!pip install huggingface-hub[cli]\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJpNHdVThg36",
        "outputId": "905d4d1e-5e7c-40e4-bef4-4910e811041d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting onnxruntime-genai\n",
            "  Downloading onnxruntime_genai-0.3.0rc2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime-genai\n",
            "Successfully installed onnxruntime-genai-0.3.0rc2\n",
            "Requirement already satisfied: huggingface-hub[cli] in /usr/local/lib/python3.10/dist-packages (0.23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[cli]) (4.12.1)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface-hub[cli])\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m804.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli])\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface-hub[cli]) (3.0.45)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub[cli]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub[cli]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub[cli]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub[cli]) (2024.6.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli]) (0.2.13)\n",
            "Installing collected packages: pfzy, InquirerPy\n",
            "Successfully installed InquirerPy-0.3.4 pfzy-0.3.4\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n",
            "Fetching 10 files:   0% 0/10 [00:00<?, ?it/s]Downloading 'cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/config.json' to 'cpu_and_mobile/.huggingface/download/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/config.json.01de35834a12bca5fc9150cc2a8351135f442757.incomplete'\n",
            "Downloading 'cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer.json' to 'cpu_and_mobile/.huggingface/download/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer.json.efc309ef56b8d8fba1b50d1b4a6e5be6cfded459.incomplete'\n",
            "\n",
            "(…)nt4-rtn-block-32-acc-level-4/config.json: 100% 919/919 [00:00<00:00, 3.73MB/s]\n",
            "Download complete. Moving file to cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/config.json\n",
            "Downloading 'cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/phi3-mini-4k-instruct-cpu-int4-rtn-block-32-acc-level-4.onnx.data' to 'cpu_and_mobile/.huggingface/download/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/phi3-mini-4k-instruct-cpu-int4-rtn-block-32-acc-level-4.onnx.data.5db30ce699aee1123cf9045742488db5928006fa618a42cb3c0840322a85ad0f.incomplete'\n",
            "Downloading 'cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/special_tokens_map.json' to 'cpu_and_mobile/.huggingface/download/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/special_tokens_map.json.32b360b36e8255e8346f50942f478e5a2227e2e6.incomplete'\n",
            "Downloading 'cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/genai_config.json' to 'cpu_and_mobile/.huggingface/download/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/genai_config.json.5e5a61eea6e20bda5b011053a889535029e4b9c1.incomplete'\n",
            "\n",
            "(…)-rtn-block-32-acc-level-4/tokenizer.json:   0% 0.00/1.84M [00:00<?, ?B/s]\u001b[ADownloading 'cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/phi3-mini-4k-instruct-cpu-int4-rtn-block-32-acc-level-4.onnx' to 'cpu_and_mobile/.huggingface/download/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/phi3-mini-4k-instruct-cpu-int4-rtn-block-32-acc-level-4.onnx.385cd1b908a0d2f8634e86d30236f6dbb7ae660eb3943fd1ef5bdc3847326480.incomplete'\n",
            "Downloading 'cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer.model' to 'cpu_and_mobile/.huggingface/download/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer.model.9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347.incomplete'\n",
            "Downloading 'cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/added_tokens.json' to 'cpu_and_mobile/.huggingface/download/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/added_tokens.json.4dece7ae8bbeb8f468cb1da428bfb6193ae0751c.incomplete'\n",
            "Downloading 'cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/configuration_phi3.py' to 'cpu_and_mobile/.huggingface/download/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/configuration_phi3.py.f4553db23ac65c608fd150a14acbd04d3ff80a0f.incomplete'\n",
            "\n",
            "\n",
            "(…)k-32-acc-level-4/special_tokens_map.json: 100% 568/568 [00:00<00:00, 3.81MB/s]\n",
            "Download complete. Moving file to cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/special_tokens_map.json\n",
            "\n",
            "\n",
            "(…)n-block-32-acc-level-4/genai_config.json: 100% 1.58k/1.58k [00:00<00:00, 10.0MB/s]\n",
            "Download complete. Moving file to cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/genai_config.json\n",
            "\n",
            "\n",
            "(…)ock-32-acc-level-4/configuration_phi3.py: 100% 10.4k/10.4k [00:00<00:00, 33.6MB/s]\n",
            "Download complete. Moving file to cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/configuration_phi3.py\n",
            "\n",
            "\n",
            "(…)n-block-32-acc-level-4/added_tokens.json: 100% 293/293 [00:00<00:00, 2.18MB/s]\n",
            "Download complete. Moving file to cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/added_tokens.json\n",
            "Fetching 10 files:  10% 1/10 [00:00<00:04,  2.18it/s]Downloading 'cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer_config.json' to 'cpu_and_mobile/.huggingface/download/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer_config.json.9d9d37222d0f5ad9b2f02408b13ec21b8023a93f.incomplete'\n",
            "\n",
            "(…)-rtn-block-32-acc-level-4/tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 12.0MB/s]\n",
            "Download complete. Moving file to cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer.json\n",
            "\n",
            "(…)ock-32-acc-level-4/tokenizer_config.json: 100% 3.17k/3.17k [00:00<00:00, 19.4MB/s]\n",
            "Download complete. Moving file to cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer_config.json\n",
            "\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:   0% 0.00/2.72G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "tokenizer.model:   0% 0.00/500k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 9.78MB/s]\n",
            "Download complete. Moving file to cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer.model\n",
            "(…)t-cpu-int4-rtn-block-32-acc-level-4.onnx: 100% 231k/231k [00:00<00:00, 5.11MB/s]\n",
            "Download complete. Moving file to cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/phi3-mini-4k-instruct-cpu-int4-rtn-block-32-acc-level-4.onnx\n",
            "Fetching 10 files:  50% 5/10 [00:00<00:00,  8.47it/s]\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:   0% 10.5M/2.72G [00:00<00:29, 90.5MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:   2% 41.9M/2.72G [00:00<00:14, 187MB/s] \u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:   3% 73.4M/2.72G [00:00<00:11, 225MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:   4% 105M/2.72G [00:00<00:10, 243MB/s] \u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:   5% 136M/2.72G [00:00<00:10, 253MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:   6% 168M/2.72G [00:00<00:09, 259MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:   7% 199M/2.72G [00:00<00:09, 265MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:   8% 231M/2.72G [00:00<00:09, 269MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  10% 262M/2.72G [00:01<00:09, 271MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  11% 294M/2.72G [00:01<00:08, 271MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  12% 325M/2.72G [00:01<00:08, 270MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  13% 357M/2.72G [00:01<00:08, 267MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  14% 388M/2.72G [00:01<00:08, 266MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  15% 419M/2.72G [00:01<00:08, 266MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  17% 451M/2.72G [00:01<00:08, 268MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  18% 482M/2.72G [00:01<00:08, 267MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  19% 514M/2.72G [00:01<00:08, 268MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  20% 545M/2.72G [00:02<00:08, 268MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  21% 577M/2.72G [00:02<00:08, 265MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  22% 608M/2.72G [00:02<00:07, 265MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  23% 640M/2.72G [00:02<00:08, 260MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  25% 671M/2.72G [00:02<00:07, 262MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  26% 703M/2.72G [00:02<00:07, 262MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  27% 734M/2.72G [00:02<00:07, 261MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  28% 765M/2.72G [00:02<00:07, 259MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  29% 797M/2.72G [00:03<00:07, 258MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  30% 828M/2.72G [00:03<00:07, 258MB/s]\u001b[A\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data:  32% 860M/2.72G [00:03<00:07, 255MB/s]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model for CPU\n",
        "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-onnx --include cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/* --local-dir ./cpu_and_mobile"
      ],
      "metadata": {
        "id": "NSLcoSlAhrUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the search options as a dictionary\n",
        "import json\n",
        "import os\n",
        "\n",
        "product_data = [\n",
        "    {\"id\": 1, \"name\": \"Tesla Model S\", \"description\": \"High-performance electric sedan with long driving range.\"},\n",
        "    {\"id\": 2, \"name\": \"Nissan Leaf\", \"description\": \"Affordable and reliable electric hatchback for daily commutes.\"},\n",
        "    {\"id\": 3, \"name\": \"Toyota Camry\", \"description\": \"Popular mid-size sedan known for its reliability and comfort.\"},\n",
        "    {\"id\": 4, \"name\": \"Chevrolet Bolt\", \"description\": \"Compact electric hatchback with impressive range and technology features.\"},\n",
        "    {\"id\": 5, \"name\": \"Ford Mustang Mach-E\", \"description\": \"Electric SUV with sporty performance and sleek design.\"},\n",
        "    {\"id\": 6, \"name\": \"Honda Accord\", \"description\": \"Well-rounded mid-size sedan with a spacious interior and fuel efficiency.\"},\n",
        "    {\"id\": 7, \"name\": \"BMW i3\", \"description\": \"Compact electric car with a unique design and premium features.\"},\n",
        "    {\"id\": 8, \"name\": \"Audi e-tron\", \"description\": \"Luxury electric SUV with advanced technology and refined craftsmanship.\"},\n",
        "    {\"id\": 9, \"name\": \"Hyundai Kona Electric\", \"description\": \"Affordable electric SUV with impressive driving range.\"},\n",
        "    {\"id\": 10, \"name\": \"Porsche Taycan\", \"description\": \"High-performance electric sports car with cutting-edge technology.\"}\n",
        "]\n",
        "\n",
        "# Specify the folder and file path\n",
        "file_path = './product_data.json'\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# Write the dictionary to a JSON file, overwriting it if it exists\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(product_data, json_file, indent=4)\n",
        "\n",
        "# Verify that the file has been created and contains the correct content\n",
        "with open(file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    print(data)\n",
        "\n",
        "\n",
        "# Define the search options as a dictionary\n",
        "search_options = {\n",
        "    \"max_length\": 2000,\n",
        "    \"num_return_sequences\": 1,\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_k\": 80,\n",
        "    \"top_p\": 0.95,\n",
        "    \"repetition_penalty\": 2.5,\n",
        "    \"do_sample\": True\n",
        "}\n",
        "\n",
        "# Specify the folder and file path\n",
        "file_path = './search_options.json'\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# Write the dictionary to a JSON file, overwriting it if it exists\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(search_options, json_file, indent=4)\n",
        "\n",
        "# Verify that the file has been created and contains the correct content\n",
        "with open(file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    print(data)\n",
        "\n",
        "\n",
        "# few_shot_examples\n",
        "few_shot_examples = [\n",
        "    {\n",
        "        \"input\": \"top car companies\",\n",
        "        \"output\": \"What are the leading car brands?\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"electric vehicles only\",\n",
        "        \"output\": \"What are the top electric vehicle brands?\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"focus on luxury EVs\",\n",
        "        \"output\": \"What are the best luxury electric vehicle brands?\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"affordable electric SUVs\",\n",
        "        \"output\": \"What are the most affordable electric SUV models?\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"compare Tesla and Audi EVs\",\n",
        "        \"output\": \"How do Tesla and Audi electric vehicles compare?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Specify the folder and file path\n",
        "file_path = './few_shot_examples.json'\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# Write the dictionary to a JSON file, overwriting it if it exists\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(few_shot_examples, json_file, indent=4)\n",
        "\n",
        "# Verify that the file has been created and contains the correct content\n",
        "with open(file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    print(data)"
      ],
      "metadata": {
        "id": "PsCAHlZkhunB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V1"
      ],
      "metadata": {
        "id": "sA_OXu_nhlDo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En6JgGeThWsD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import time\n",
        "import onnxruntime_genai as og\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class ContextMemory:\n",
        "    \"\"\"A class to manage the conversation history and provide the relevant context.\"\"\"\n",
        "    def __init__(self, max_history_length: int = 5):\n",
        "        self.memory: List[str] = []\n",
        "        self.max_history_length: int = max_history_length\n",
        "\n",
        "    def update_memory(self, user_prompt: str) -> None:\n",
        "        \"\"\"Updates the conversation history by adding the user prompt and limiting the history length.\"\"\"\n",
        "        self.memory.append(f\"<|user|>\\n{user_prompt}<|end|>\")\n",
        "        if len(self.memory) > self.max_history_length:\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    def get_context(self) -> str:\n",
        "        \"\"\"Retrieves the conversation history as a string.\"\"\"\n",
        "        return \"\".join(self.memory)\n",
        "\n",
        "class EmbeddingProvider:\n",
        "    \"\"\"A class to generate embeddings for text queries using a pre-trained sentence transformer model.\"\"\"\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def get_embedding(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Generates the embedding for a given text query.\"\"\"\n",
        "        return self.model.encode(query)\n",
        "\n",
        "class FaissIndexManager:\n",
        "    \"\"\"A class to create and search a FAISS index for efficient similarity search.\"\"\"\n",
        "    def __init__(self, embedding_size: int, M: int = 16, ef_construction: int = 200, ef_search: int = 200):\n",
        "        self.index = faiss.IndexHNSWFlat(embedding_size, M)\n",
        "        self.index.hnsw.efConstruction = ef_construction\n",
        "        self.index.hnsw.efSearch = ef_search\n",
        "        self.product_embeddings: List[np.ndarray] = []\n",
        "        self.product_ids: List[int] = []\n",
        "\n",
        "    def add_items(self, item_embeddings: np.ndarray, item_ids: List[int]) -> None:\n",
        "        self.index.add(item_embeddings)\n",
        "        self.product_embeddings.extend(item_embeddings)\n",
        "        self.product_ids.extend(item_ids)\n",
        "\n",
        "    def update_items(self, item_embeddings: np.ndarray, item_ids: List[int]) -> None:\n",
        "        for item_id, item_embedding in zip(item_ids, item_embeddings):\n",
        "            if item_id in self.product_ids:\n",
        "                idx = self.product_ids.index(item_id)\n",
        "                self.product_embeddings[idx] = item_embedding\n",
        "            else:\n",
        "                self.add_items(np.array([item_embedding]), [item_id])\n",
        "        self.index.reset()\n",
        "        self.index.add(np.array(self.product_embeddings))\n",
        "\n",
        "    def search(self, query_embedding: np.ndarray, top_k: int = 3) -> List[int]:\n",
        "        _, indices = self.index.search(query_embedding, top_k)\n",
        "        return [self.product_ids[idx] for idx in indices[0]]\n",
        "\n",
        "class SearchModel:\n",
        "    \"\"\"A class to generate reformulated queries based on user input and conversation history.\"\"\"\n",
        "    def __init__(self, model_path: str, search_options: Dict[str, Any], few_shot_examples_path: str, product_type: str, max_history_length: int = 5):\n",
        "        self.model = og.Model(model_path)\n",
        "        self.tokenizer = og.Tokenizer(self.model)\n",
        "        self.search_options = search_options\n",
        "        self.context_memory = ContextMemory(max_history_length)\n",
        "        self.few_shot_examples = self.load_few_shot_examples(few_shot_examples_path)\n",
        "        self.product_type = product_type\n",
        "\n",
        "    def load_few_shot_examples(self, file_path: str) -> List[Dict[str, str]]:\n",
        "        with open(file_path, 'r') as file:\n",
        "            return json.load(file)\n",
        "\n",
        "    def generate_reformulated_query(self, input_text: str) -> str:\n",
        "        \"\"\"Generates a reformulated query based on the user input and conversation history.\"\"\"\n",
        "        conversation_history = self.context_memory.get_context()\n",
        "        self.context_memory.update_memory(input_text)\n",
        "\n",
        "        few_shot_prompt = \"\"\n",
        "        for example in self.few_shot_examples:\n",
        "            few_shot_prompt += f\"<|user|>\\n{example['input']}\\n<|assistant|>\\n{example['output']}\\n<|end|>\\n\"\n",
        "\n",
        "        if conversation_history.strip():\n",
        "            context_prompt = f\"Conversation History:\\n{conversation_history}\\n\\n\"\n",
        "        else:\n",
        "            context_prompt = \"No previous conversation history.\\n\\n\"\n",
        "\n",
        "        full_prompt = f\"{few_shot_prompt}<|user|>\\nAs an AI assistant, your task is to reformulate the user's input into a highly specific question about cars. Focus on the key details provided by the user and generate a question that targets their precise needs or preferences.\\n\\n{context_prompt}User Input: {input_text}\\n\\nInstructions:\\n- Carefully analyze the user's input and identify the most important details or criteria.\\n- Consider the conversation history to understand the context and the user's evolving preferences.\\n- Reformulate the input into a clear, concise, and highly specific question that directly addresses the user's needs.\\n- Avoid generating broad or generic questions. Focus on the unique aspects mentioned by the user.\\n- Use the question to guide the search towards the most relevant car recommendations.\\n\\n<|end|><|user|>\\n{input_text}<|end|><|assistant|>\"\n",
        "\n",
        "        input_tokens = self.tokenizer.encode(full_prompt)\n",
        "        params = og.GeneratorParams(self.model)\n",
        "        params.set_search_options(**self.search_options)\n",
        "        params.input_ids = input_tokens\n",
        "        generator = og.Generator(self.model, params)\n",
        "        response_tokens = []\n",
        "        while not generator.is_done():\n",
        "            generator.compute_logits()\n",
        "            generator.generate_next_token()\n",
        "            new_token = generator.get_next_tokens()[0]\n",
        "            response_tokens.append(new_token)\n",
        "        reformulated_query = self.tokenizer.decode(response_tokens)\n",
        "        reformulated_query = reformulated_query.split('<|assistant|>')[-1].strip()\n",
        "        return reformulated_query\n",
        "\n",
        "def load_json_file(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Loads a JSON file and returns its contents as a dictionary.\"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "def load_product_data(file_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Loads product data from a JSON file.\"\"\"\n",
        "    return load_json_file(file_path)\n",
        "\n",
        "def load_search_options(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Loads search options from a JSON file.\"\"\"\n",
        "    return load_json_file(file_path)\n",
        "\n",
        "def process_user_input(user_input: str, conv_model: SearchModel, embedding_provider: EmbeddingProvider, faiss_index_manager: FaissIndexManager, products: List[Dict[str, Any]]) -> None:\n",
        "    \"\"\"Processes a single user input and generates a response.\"\"\"\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        reformulated_query = conv_model.generate_reformulated_query(user_input)\n",
        "        query_embedding = embedding_provider.get_embedding(reformulated_query).reshape(1, -1)\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "\n",
        "        print(\"Assistant:\", reformulated_query)\n",
        "        print(f\"Execution time: {execution_time:.5f} seconds\")\n",
        "\n",
        "        # Print the current state of the conversation history\n",
        "        print(\"Current Conversation History:\")\n",
        "        print(conv_model.context_memory.get_context())\n",
        "\n",
        "        # Retrieve top matching product IDs\n",
        "        top_product_ids = faiss_index_manager.search(query_embedding)\n",
        "        print(\"Top matches:\")\n",
        "        for product_id in top_product_ids:\n",
        "            product = next(product for product in products if product['id'] == product_id)\n",
        "            print(f\"  - ID: {product['id']}, Name: {product['name']}, Description: {product['description']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating response: {str(e)}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"The main function that orchestrates the search process.\"\"\"\n",
        "    # Load search options, product data, and few-shot examples\n",
        "    search_options_path = './search_options.json'\n",
        "    product_data_path = './product_data.json'\n",
        "    few_shot_examples_path = './few_shot_examples.json'\n",
        "    search_options = load_search_options(search_options_path)\n",
        "    products = load_product_data(product_data_path)\n",
        "    product_descriptions = [product['description'] for product in products]\n",
        "\n",
        "    # Initialize the model and components\n",
        "    model_path = './cpu_and_mobile/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4'\n",
        "    product_type = \"car\"  # Replace with the appropriate product type\n",
        "    conv_model = SearchModel(model_path, search_options, few_shot_examples_path, product_type, max_history_length=5)\n",
        "    embedding_provider = EmbeddingProvider('all-mpnet-base-v2')\n",
        "    product_embeddings = np.array([embedding_provider.get_embedding(product['description']) for product in products])\n",
        "    faiss_index_manager = FaissIndexManager(embedding_size=product_embeddings.shape[1])\n",
        "    faiss_index_manager.add_items(product_embeddings, [product['id'] for product in products])\n",
        "\n",
        "    # Pre-load models and data\n",
        "    _ = conv_model.generate_reformulated_query(\"Dummy input\")\n",
        "    _ = embedding_provider.get_embedding(\"Dummy query\")\n",
        "    conv_model.context_memory.memory.clear()  # Clear the dummy input from the conversation history\n",
        "\n",
        "    # Generation loop\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        while True:\n",
        "            user_input = input(\"User: \").strip()\n",
        "            if user_input.lower() == 'exit':\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                print(\"Please enter a valid input.\")\n",
        "                continue\n",
        "\n",
        "            executor.submit(process_user_input, user_input, conv_model, embedding_provider, faiss_index_manager, products)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PW0_5nKQhxbP"
      }
    }
  ]
}